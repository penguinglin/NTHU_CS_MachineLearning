{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "# IMAGE_SIZE = 224 #EfficientNetB0, DenseNet121\n",
    "IMAGE_SIZE = 448 # EfficientNetV2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['./offline-train.pkl','./train_caption.txt']\n",
    "valid_datasets=['./offline-test.pkl', './test_caption.txt']\n",
    "dictionaries=['./dictionary.txt']\n",
    "batch_Imagesize=500000\n",
    "valid_batch_Imagesize=500000\n",
    "# batch_size for training and testing\n",
    "batch_size=32\n",
    "# the max (label length/Image size) in training and testing\n",
    "# you can change 'maxlen','maxImagesize' by the size of your GPU\n",
    "maxlen=48\n",
    "maxImagesize= 100000\n",
    "# hidden_size in RNN\n",
    "hidden_size = 256\n",
    "# teacher_forcing_ratio \n",
    "teacher_forcing_ratio = 1\n",
    "\n",
    "AUG = False\n",
    "AUG_NUM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as A\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words/phones 112\n",
      "total words/phones 112\n"
     ]
    }
   ],
   "source": [
    "fp=open(dictionaries[0])\n",
    "\n",
    "symbol_index={}\n",
    "\n",
    "for line in fp.readlines():\n",
    "    words = line.strip().split()\n",
    "    symbol_index[words[0]]=int(words[1])  # word->index\n",
    "\n",
    "fp.close()\n",
    "\n",
    "print('total words/phones',len(symbol_index))\n",
    "\n",
    "index_symbol={}\n",
    "for k,v in symbol_index.items():\n",
    "    index_symbol[v]=k  # index->word\n",
    "    \n",
    "print('total words/phones',len(index_symbol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels 8835\n",
      "caption_list[0] ['f', '(', 'x', ')', '=', '\\\\sum', '_', '{', 'n', '=', '-', '\\\\infty', '}', '^', '{', '\\\\infty', '}', 'a', '_', '{', 'n', '}', '=', 'x', '^', '{', '\\\\alpha', '_', '{', 'n', '}', '}']\n"
     ]
    }
   ],
   "source": [
    "label_fp = open(datasets[1], 'r')\n",
    "labels = label_fp.readlines()\n",
    "label_fp.close()\n",
    "label_num = len(labels)\n",
    "print('total labels',label_num)\n",
    "\n",
    "caption_list = []\n",
    "\n",
    "for i in range(label_num):\n",
    "\n",
    "    label = labels[i].strip().split()\n",
    "    file_id = label[0]\n",
    "    label = label[1:]\n",
    "    caption_list.append(label)\n",
    "        \n",
    "# caption_list = np.array(caption_list)\n",
    "print('caption_list[0]',caption_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./off_image_train/formulaire004-equation047_0.jpg\n",
      "<start> f ( x ) = \\sum _ { n = - \\infty } ^ { \\infty } a _ { n } = x ^ { \\alpha _ { n } } <end>\n"
     ]
    }
   ],
   "source": [
    "img_name_list = []\n",
    "# caption_list = []\n",
    "i = 0\n",
    "with open('./train_caption.txt') as fh:\n",
    "    for line in fh:\n",
    "        image_name = line.strip().split()[0]\n",
    "        caption = caption_list[i]\n",
    "        img_name_list.append(f'./off_image_train/{image_name}_0.jpg')\n",
    "        caption_list[i] = ('<start> ' + ' '.join(caption) + ' <end>')\n",
    "        i += 1\n",
    "        \n",
    "# Only first 120,000 images have labels\n",
    "\n",
    "# test_img_name = set(glob(f'./words_captcha/*.png')) - set(img_name_list)\n",
    "# img_name_list += sorted(test_img_name)\n",
    "\n",
    "print(img_name_list[0])\n",
    "print(caption_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> f ( x ) = \\sum _ { n = - \\infty } ^ { \\infty } a _ { n } = x ^ { \\alpha _ { n } } <end>\n",
      "[ 4 30 13 10 14 16 39 11  2 19 16 12 53  3  8  2 53  3 17 11  2 19  3 16\n",
      " 10  8  2 52 11  2 19  3  3  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='', filters=' ')\n",
    "# tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
    "tokenizer.fit_on_texts(caption_list)\n",
    "caption_seq = tokenizer.texts_to_sequences(caption_list)\n",
    "\n",
    "# iterate over the whole dataset if its caption over 48 words, remove it\n",
    "\n",
    "tmp_image_name_list = []\n",
    "tmp_caption_seq = []\n",
    "\n",
    "for i in range(len(caption_seq)):\n",
    "    if len(caption_seq[i]) <= 48:\n",
    "        tmp_image_name_list.append(img_name_list[i])\n",
    "        tmp_caption_seq.append(caption_seq[i])\n",
    "\n",
    "img_name_list = tmp_image_name_list\n",
    "caption_seq = tmp_caption_seq\n",
    "\n",
    "caption_seq = tf.keras.preprocessing.sequence.pad_sequences(caption_seq, padding='post')\n",
    "max_length = len(caption_seq[0])\n",
    "\n",
    "print(caption_list[0])\n",
    "print(caption_seq[0])\n",
    "print (max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption_seq_train[0] [ 4 30 13 10 14 16 39 11  2 19 16 12 53  3  8  2 53  3 17 11  2 19  3 16\n",
      " 10  8  2 52 11  2 19  3  3  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "img_name_train[0] ./off_image_train/formulaire004-equation047_0.jpg\n"
     ]
    }
   ],
   "source": [
    "img_name_train = img_name_list[:6500]\n",
    "img_name_valid = img_name_list[6500:]\n",
    "\n",
    "caption_seq_train = caption_seq[:6500]\n",
    "caption_seq_valid = caption_seq[6500:]\n",
    "\n",
    "# img_name_test = img_name_list[120000:]\n",
    "# caption_seq_test = caption_seq[120000:]\n",
    "\n",
    "# print the first caption and image name\n",
    "print('caption_seq_train[0]',caption_seq_train[0])\n",
    "print('img_name_train[0]',img_name_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not exist ./off_image_train/formulaire004-equation047_0.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in range(len(img_name_train)):\n",
    "    if not os.path.exists(img_name_train[i]):\n",
    "        print('not exist',img_name_train[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 # 100 is a suck number\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, caption):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # img = tf.image.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    # Let's try to use padding instead of resize\n",
    "    img = tf.image.resize_with_pad(img, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    \n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "    \n",
    "    # image = tf.keras.applications.densenet.preprocess_input(img)\n",
    "    # image = tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "    image = tf.keras.applications.efficientnet_v2.preprocess_input(img)\n",
    "    return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, caption_seq_train))\\\n",
    "                               .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_valid, caption_seq_valid))\\\n",
    "                               .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.EfficientNetV2M(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "# image_model = tf.keras.applications.DenseNet121(include_top=False,\n",
    "#                                                 weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "feature_extractor = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN_Decoder(tf.keras.Model):\n",
    "#     def __init__(self, embedding_dim, units, vocab_size):\n",
    "#         super(RNN_Decoder, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "#                                          return_sequences=True,\n",
    "#                                          return_state=True,\n",
    "#                                          recurrent_initializer='glorot_uniform')\n",
    "#         self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "#         self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "#         self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "#     def call(self, x, features, hidden):\n",
    "#         # defining attention as a separate model\n",
    "#         context_vector, attention_weights = self.attention(features, hidden[0])\n",
    "\n",
    "#         # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "#         # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "#         x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "#         # passing the concatenated vector to the LSTM\n",
    "#         output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
    "\n",
    "#         # shape == (batch_size, max_length, hidden_size)\n",
    "#         x = self.fc1(output)\n",
    "\n",
    "#         # x shape == (batch_size * max_length, hidden_size)\n",
    "#         x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "#         # output shape == (batch_size * max_length, vocab)\n",
    "#         x = self.fc2(x)\n",
    "\n",
    "#         return x, [state_h, state_c], attention_weights\n",
    "\n",
    "#     def reset_state(self, batch_size):\n",
    "#         return [tf.zeros((batch_size, self.units)), tf.zeros((batch_size, self.units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = tf.keras.layers.GRU(self.units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "        self.gru2 = tf.keras.layers.GRU(self.units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden[0])\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the first GRU\n",
    "        output, state = self.gru1(x, initial_state=hidden)\n",
    "\n",
    "        # passing the output of the first GRU to the second GRU\n",
    "        output, state = self.gru2(output, initial_state=state)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # applying dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, [state], attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return [tf.zeros((batch_size, self.units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = r\"C:\\Users\\a2004\\Downloads\\ML_portable\\checkpoints\\train\"\n",
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\a2004\\\\Downloads\\\\ML_portable\\\\checkpoints\\\\train\\\\ckpt-49'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "\n",
    "ckpt_manager.restore_or_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "valid_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    # tf.compat.v1.enable_eager_execution()\n",
    "    \n",
    "    # plt.imshow(img_tensor[0].numpy())\n",
    "    # plt.show()\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        #  Instead of prestore the features into npy file, we called the feature_extractor directly (Easier to implement lol)\n",
    "        features = feature_extractor(img_tensor, True)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "    \n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(args):\n",
    "    results = []\n",
    "    for i in args:\n",
    "        result = \"\"\n",
    "        for s in i[1:]:\n",
    "            if s == tokenizer.word_index[\"<end>\"]:\n",
    "                break\n",
    "            else :\n",
    "                result += (tokenizer.index_word[s] + \" \")\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_valid):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for img_tensor, target in dataset_valid:\n",
    "        \n",
    "        pred_list = build_output(predict(img_tensor).numpy())\n",
    "        real_list = build_output(target.numpy())\n",
    "        \n",
    "        total += min(len(pred_list), len(real_list))\n",
    "        correct += sum([1 for pred, real in zip(pred_list, real_list) if pred == real]) \n",
    "        \n",
    "        # if(pred_list == real_list):\n",
    "        #     correct += 1\n",
    "        # total += 1\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 0 epoch 0.0 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "EPOCHS = 0\n",
    "start = time.time()\n",
    "\n",
    "max_accuracy = 0.0\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in tqdm(enumerate(dataset_train), total=num_steps):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    \n",
    "    accuracy = evaluate(dataset_valid)\n",
    "    valid_plot.append(accuracy)\n",
    "    \n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f} Valid {:.3f} History Valid {:.3f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps, accuracy, max_accuracy))\n",
    "    \n",
    "    if(accuracy >= 0.90):\n",
    "        break\n",
    "    \n",
    "print ('Time taken for {} epoch {} sec\\n'.format(EPOCHS, time.time() - start))\n",
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot, label='Training Loss')\n",
    "plt.plot(valid_plot, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training Loss and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "#                            encoder=encoder,\n",
    "#                            decoder=decoder,\n",
    "#                            optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt.restore('./checkpoints/train/ckpt-33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = evaluate(dataset_valid)\n",
    "# print(f'Final Validation Accuracy: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    # img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    image = tf.keras.applications.efficientnet_v2.preprocess_input(img)\n",
    "    return image, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_test = [\"./2+4.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
    "                              .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(BATCH_SIZE)\\\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:02<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'./2+4.jpg'\n",
      " \\int \\sin \\beta d x \n",
      "Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\int \\sin \\beta d x $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Latex\n",
    "\n",
    "ans = []\n",
    "\n",
    "with open('./output.txt', 'w') as fh:\n",
    "    for img_tensor, img_path in tqdm(dataset_test):\n",
    "        pred_list = build_output(predict(img_tensor).numpy())\n",
    "        for path, pred in zip(img_path, pred_list):\n",
    "            # img_name = re.search(r'(\\w+)\\._0.jpg', path.numpy().decode()).group(1)\n",
    "            img_name = path\n",
    "            fh.write(f'{img_name} {pred}\\n')\n",
    "            print(f'{img_name}\\n {pred}')\n",
    "            ans.append(pred)\n",
    "            \n",
    "# Latex(\"$\" + r\"\\frac{a}{\\sin x}+\\frac{p}{b^{x}}\" + \"$\")\n",
    "print(\"Predictions:\")\n",
    "Latex(\"$\" + pred + \"$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "\n",
    "# Latex(\"$\" + r\"\\frac{a}{\\sin9}=\\frac{b}{\\sin4}\" + \"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Attention gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_extract_model = tf.keras.applications.EfficientNetV2M(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "attention_features_shape = 196\n",
    "\n",
    "def evaluate_1(img_path):\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "    \n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "    \n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        \n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "        \n",
    "    attention_plot = attention_plot[:len(result[0]), :]\n",
    "\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Caption: [ 4 43 33 63 27 10  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5]\n",
      "Prediction Caption: ['\\\\int', '\\\\sin', '\\\\beta', 'd', 'x']\n"
     ]
    }
   ],
   "source": [
    "result, attention_plot = evaluate_1(img_name_test[0])\n",
    "\n",
    "# print('Prediction Caption:', (result[0]))\n",
    "\n",
    "# transform sysbol to latex\n",
    "\n",
    "result = result[0].numpy()\n",
    "string = []\n",
    "\n",
    "print('Prediction Caption:', (result))\n",
    "\n",
    "for i in range(len(result)):\n",
    "    if result[i] == tokenizer.word_index['<end>']:\n",
    "        string.append(tokenizer.index_word[result[i]])\n",
    "        break\n",
    "    # print(tokenizer.index_word[result[i]], end=' ')\n",
    "    string.append(tokenizer.index_word[result[i]])\n",
    "\n",
    "string = string[1:-1]\n",
    "print('Prediction Caption:', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# def plot_attention(image, result, attention_plot):\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     temp_image = np.array(Image.open(image))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# plot_attention(img_name_test[0], string, attention_plot)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_gif\u001b[39m(image_path, result, attention_plot, gif_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention.gif\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "# def plot_attention(image, result, attention_plot):\n",
    "#     temp_image = np.array(Image.open(image))\n",
    "\n",
    "#     fig = plt.figure(figsize=(100, 100))\n",
    "\n",
    "#     len_result = len(result)\n",
    "#     for l in range(len_result):\n",
    "#         temp_att = np.resize(attention_plot[l], (10, 10))\n",
    "#         ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "#         ax.set_title(result[l])\n",
    "#         img = ax.imshow(temp_image)\n",
    "#         ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "# plt.imshow(np.array(Image.open(img_name_test[0])))\n",
    "\n",
    "# plot_attention(img_name_test[0], string, attention_plot)\n",
    "\n",
    "def generate_gif(image_path, result, attention_plot, gif_path='attention.gif'):\n",
    "    temp_image = np.array(Image.open(image_path))\n",
    "    frames = []\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Save the current frame\n",
    "        fig.canvas.draw()\n",
    "        frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        frames.append(frame)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Save frames as a gif\n",
    "    imageio.mimsave(gif_path, frames, fps=5)\n",
    "\n",
    "generate_gif(img_name_test[0], string, attention_plot)\n",
    "# display the gif\n",
    "from IPython.display import Image\n",
    "Image(filename='attention.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
